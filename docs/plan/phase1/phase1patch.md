Excellent.
We now move into **Phase 1: Indexing Engine patch proposals** â€” the **backbone of LocalPilot**.

Below is **Phase 1, Tasks 1.1 â†’ 1.4** as **patch proposals**, following **exactly** the agent constitution you approved:

* patch-driven
* deterministic
* no shortcuts
* no AI generation logic yet (only plumbing + deterministic algorithms)
* future-proof for v1.1 / v1.2

> âš ï¸ **Important scope note**
> I am intentionally splitting Phase 1 into **controlled batches**.
> This message covers **Index Discovery â†’ Parsing â†’ Chunking (no embeddings yet)**.
> This keeps risk low and patches reviewable.

---

# ğŸ§© PHASE 1 â€” PATCH PROPOSALS (PART 1)

## Indexing Engine: Scan â†’ Parse â†’ Chunk

---

## ğŸ§± TASK 1.1 â€” Workspace Scanner

### ğŸ¯ Objective

Deterministically discover indexable files while respecting user intent.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.1",
  "title": "Implement workspace file scanner",
  "files": [
    { "path": "server/indexing/scanner.py", "action": "create" },
    { "path": "server/indexing/__init__.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium",
  "rollback": "safe-delete"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/scanner.py
+from pathlib import Path
+from typing import List
+
+EXCLUDED_DIRS = {
+    ".git",
+    "node_modules",
+    "dist",
+    "build",
+    ".venv",
+    "__pycache__",
+    ".localpilot"
+}
+
+SUPPORTED_EXTENSIONS = {
+    ".ts", ".js", ".py", ".json", ".md", ".dart"
+}
+
+
+class WorkspaceScanner:
+    def scan(self, root: Path) -> List[Path]:
+        files: List[Path] = []
+
+        for path in root.rglob("*"):
+            if not path.is_file():
+                continue
+
+            if any(part in EXCLUDED_DIRS for part in path.parts):
+                continue
+
+            if path.suffix not in SUPPORTED_EXTENSIONS:
+                continue
+
+            files.append(path)
+
+        return sorted(files)
```

---

### âœ… Verification Checklist

* [ ] Skips excluded directories
* [ ] Deterministic ordering
* [ ] No file reads yet
* [ ] No gitignore parsing yet (v1.1)

---

## ğŸ§± TASK 1.2 â€” Language Detection

### ğŸ¯ Objective

Determine programming language **deterministically** from file extension.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.2",
  "title": "Add language detection utility",
  "files": [
    { "path": "server/indexing/language.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "low"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/language.py
+from pathlib import Path
+
+
+EXTENSION_LANGUAGE_MAP = {
+    ".ts": "typescript",
+    ".js": "javascript",
+    ".py": "python",
+    ".dart": "dart",
+    ".json": "json",
+    ".md": "markdown"
+}
+
+
+def detect_language(path: Path) -> str | None:
+    return EXTENSION_LANGUAGE_MAP.get(path.suffix)
```

---

### âœ… Verification Checklist

* [ ] Deterministic mapping
* [ ] Returns None for unsupported files
* [ ] No heuristics

---

## ğŸ§± TASK 1.3 â€” AST Parsing Abstraction (Tree-sitter Ready)

### ğŸ¯ Objective

Create a **parser abstraction**, without binding to Tree-sitter yet.

This keeps Phase 1 safe and testable.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.3",
  "title": "Introduce parser abstraction and base parser",
  "files": [
    { "path": "server/indexing/parsers/base.py", "action": "create" },
    { "path": "server/indexing/parsers/__init__.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/parsers/base.py
+from pathlib import Path
+from typing import Any
+
+
+class ParseResult:
+    def __init__(self, ast: Any, source: str):
+        self.ast = ast
+        self.source = source
+
+
+class BaseParser:
+    language: str
+
+    def parse(self, path: Path) -> ParseResult:
+        raise NotImplementedError("Parser must implement parse()")
```

---

### âœ… Verification Checklist

* [ ] No Tree-sitter dependency yet
* [ ] Abstract contract only
* [ ] Safe to extend later

---

## ğŸ§± TASK 1.4 â€” Semantic Chunk Model

### ğŸ¯ Objective

Define **what a chunk is** â€” before generating embeddings.

This is critical for RAG quality.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.4",
  "title": "Define semantic chunk data model",
  "files": [
    { "path": "server/indexing/chunk.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium"
}
```

---

### ğŸ§¾ Unified Diff

````diff
+++ server/indexing/chunk.py
+from dataclasses import dataclass
+
+
+@dataclass(frozen=True)
+class CodeChunk:
+    id: str
+    file_path: str
+    language: str
+    start_line: int
+    end_line: int
+    content: str
+    symbol_type: str  # function, class, module, block
+```

---

### âœ… Verification Checklist
- [ ] Immutable chunk
- [ ] Line-accurate metadata
- [ ] No embeddings yet
- [ ] No vector DB coupling

---

## ğŸ”’ Phase 1 (Part 1) Status

At this point, you have:

âœ… Deterministic workspace scanning  
âœ… Explicit language detection  
âœ… Parser abstraction (no premature complexity)  
âœ… Canonical chunk model  

ğŸš« Still intentionally missing:
- embeddings
- ChromaDB
- Ollama calls
- RAG queries

This is **exactly correct**.

---

## ğŸ”œ Next Step (Phase 1 â€“ Part 2)

Next batch will cover:

- **Task 1.5** â€” Semantic chunking logic (AST â†’ chunks)
- **Task 1.6** â€” Hash tracking (sync foundation)
- **Task 1.7** â€” Index state persistence
- **Task 1.8** â€” Progress reporting

Say when ready:

ğŸ‘‰ **â€œContinue Phase 1 patch proposals (Part 2)â€**

Youâ€™re building this the *right* way.
````

Perfect.
This is **Phase 1 â€“ Part 2**, completing the **Indexing Engine core (still no embeddings yet)**.

Scope of this message:

* Semantic chunking logic (AST â†’ chunks)
* Hash tracking (for smart sync)
* Index state persistence
* Progress reporting (server-side)

Still **NO Ollama**, **NO ChromaDB**, **NO RAG queries**.
We are finishing the *deterministic backbone* first.

---

# ğŸ§© PHASE 1 â€” PATCH PROPOSALS (PART 2)

## Chunking â†’ Sync â†’ Persistence â†’ Progress

---

## ğŸ§± TASK 1.5 â€” Semantic Chunker (AST â†’ CodeChunks)

### ğŸ¯ Objective

Convert parsed source into **stable, semantically meaningful chunks**.

> âš ï¸ Important:
> This implementation is **language-agnostic fallback chunking**.
> Tree-sitterâ€“aware chunking will *extend* this in v1.1, not replace it.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.5",
  "title": "Implement semantic chunker",
  "files": [
    { "path": "server/indexing/chunker.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium",
  "rollback": "safe-delete"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/chunker.py
+import hashlib
+from typing import List
+
+from .chunk import CodeChunk
+
+
+class SemanticChunker:
+    def chunk_file(
+        self,
+        file_path: str,
+        language: str,
+        source: str
+    ) -> List[CodeChunk]:
+        """
+        Fallback semantic chunking:
+        - Entire file = single chunk
+        - Used until AST-aware chunkers are plugged in
+        """
+
+        lines = source.splitlines()
+        content = source.strip()
+
+        chunk_id = self._stable_id(file_path, content)
+
+        return [
+            CodeChunk(
+                id=chunk_id,
+                file_path=file_path,
+                language=language,
+                start_line=1,
+                end_line=len(lines),
+                content=content,
+                symbol_type="file"
+            )
+        ]
+
+    def _stable_id(self, file_path: str, content: str) -> str:
+        h = hashlib.sha256()
+        h.update(file_path.encode("utf-8"))
+        h.update(content.encode("utf-8"))
+        return h.hexdigest()
```

---

### âœ… Verification Checklist

* [ ] One chunk per file (for now)
* [ ] Stable chunk IDs
* [ ] Line numbers correct
* [ ] No AST dependency yet

---

## ğŸ§± TASK 1.6 â€” File Hash Tracker (Smart Sync Foundation)

### ğŸ¯ Objective

Track file changes deterministically for **smart re-indexing**.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.6",
  "title": "Add file hash tracker",
  "files": [
    { "path": "server/indexing/hash_tracker.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/hash_tracker.py
+import hashlib
+from pathlib import Path
+
+
+def hash_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
```

---

### âœ… Verification Checklist

* [ ] Deterministic hash
* [ ] Handles large files
* [ ] No global state

---

## ğŸ§± TASK 1.7 â€” Index State Persistence

### ğŸ¯ Objective

Persist index metadata to disk so indexing survives restarts.

ğŸ“ Location (locked earlier):

```
~/.localpilot/indexes/{projectId}/state.json
```

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.7",
  "title": "Persist index state to disk",
  "files": [
    { "path": "server/indexing/state.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/state.py
+import json
+from pathlib import Path
+from typing import Dict
+
+
+class IndexState:
+    def __init__(self, root: Path):
+        self.path = root / "state.json"
+        self.file_hashes: Dict[str, str] = {}
+
+    def load(self) -> None:
+        if not self.path.exists():
+            return
+        with open(self.path, "r", encoding="utf-8") as f:
+            data = json.load(f)
+            self.file_hashes = data.get("file_hashes", {})
+
+    def save(self) -> None:
+        self.path.parent.mkdir(parents=True, exist_ok=True)
+        with open(self.path, "w", encoding="utf-8") as f:
+            json.dump(
+                {"file_hashes": self.file_hashes},
+                f,
+                indent=2
+            )
```

---

### âœ… Verification Checklist

* [ ] Creates directory if missing
* [ ] JSON readable
* [ ] Safe when state absent

---

## ğŸ§± TASK 1.8 â€” Indexing Progress Reporter

### ğŸ¯ Objective

Expose **structured progress events** to the extension UI.

> No WebSockets yet â€” this is an internal abstraction.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.8",
  "title": "Add indexing progress tracker",
  "files": [
    { "path": "server/indexing/progress.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "low"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/progress.py
+from typing import Callable, Literal
+
+Phase = Literal["scan", "parse", "chunk", "hash", "complete"]
+
+
+class ProgressTracker:
+    def __init__(self, callback: Callable[[Phase, int, int], None]):
+        self.callback = callback
+
+    def report(self, phase: Phase, current: int, total: int) -> None:
+        self.callback(phase, current, total)
```

---

### âœ… Verification Checklist

* [ ] No side effects
* [ ] Phase-limited enum
* [ ] UI-agnostic

---

## ğŸ”’ Phase 1 (Parts 1 + 2) â€” CURRENT STATE

You now have:

âœ… Workspace scanning
âœ… Language detection
âœ… Parser abstraction
âœ… Canonical chunk model
âœ… Deterministic chunking
âœ… Hash-based sync foundation
âœ… Persistent index state
âœ… Progress reporting abstraction

ğŸš« Still NOT present (by design):

* embeddings
* Ollama calls
* vector database
* RAG querying
* summary generation

This is **exactly where you want to be** before introducing ML.

---

Excellent.
This is **Phase 1 â€“ Part 3**, where we **carefully introduce embeddings and vector storage**, while still keeping everything **deterministic, auditable, and MVP-safe**.

Scope of this message:

* Embedding abstraction (port)
* Ollama embedding implementation
* ChromaDB vector storage
* Index orchestration service (the glue)

This **completes Phase 1**.

---

# ğŸ§© PHASE 1 â€” PATCH PROPOSALS (PART 3)

## Embeddings â†’ Vector Store â†’ Index Orchestration

---

## ğŸ§± TASK 1.9 â€” Embedding Provider Abstraction

### ğŸ¯ Objective

Define a **clean embedding contract** so the rest of the system does not depend on Ollama directly.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.9",
  "title": "Define embedding provider interface",
  "files": [
    { "path": "server/indexing/embeddings/base.py", "action": "create" },
    { "path": "server/indexing/embeddings/__init__.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "medium"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/embeddings/base.py
+from abc import ABC, abstractmethod
+from typing import List
+
+
+class EmbeddingProvider(ABC):
+    @abstractmethod
+    def embed(self, texts: List[str]) -> List[List[float]]:
+        """
+        Generate embeddings for a list of texts.
+        Must be deterministic for identical inputs.
+        """
+        raise NotImplementedError
```

---

### âœ… Verification Checklist

* [ ] No Ollama import
* [ ] Batch-based API
* [ ] Deterministic contract

---

## ğŸ§± TASK 1.10 â€” Ollama Embedding Implementation

### ğŸ¯ Objective

Implement embeddings using **Ollama**, without leaking Ollama details upward.

ğŸ“Œ Model choice stays configurable later (v1.1).

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.10",
  "title": "Implement Ollama embedding provider",
  "files": [
    { "path": "server/indexing/embeddings/ollama.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "high"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/embeddings/ollama.py
+import requests
+from typing import List
+
+from .base import EmbeddingProvider
+
+
+class OllamaEmbeddingProvider(EmbeddingProvider):
+    def __init__(self, base_url: str, model: str):
+        self.base_url = base_url.rstrip("/")
+        self.model = model
+
+    def embed(self, texts: List[str]) -> List[List[float]]:
+        embeddings: List[List[float]] = []
+
+        for text in texts:
+            res = requests.post(
+                f"{self.base_url}/api/embeddings",
+                json={
+                    "model": self.model,
+                    "prompt": text
+                },
+                timeout=60
+            )
+            res.raise_for_status()
+            data = res.json()
+            embeddings.append(data["embedding"])
+
+        return embeddings
```

---

### âœ… Verification Checklist

* [ ] Uses `/api/embeddings`
* [ ] No streaming
* [ ] One embedding per chunk
* [ ] Raises on failure (no silent errors)

---

## ğŸ§± TASK 1.11 â€” Vector Store (ChromaDB)

### ğŸ¯ Objective

Persist embeddings + metadata in a **queryable vector database**.

ğŸ“Œ Storage location:

```
~/.localpilot/indexes/{projectId}/chroma/
```

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.11",
  "title": "Add ChromaDB vector store",
  "files": [
    { "path": "server/indexing/vector_store.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "high"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/vector_store.py
+from typing import List
+
+import chromadb
+
+from .chunk import CodeChunk
+
+
+class VectorStore:
+    def __init__(self, persist_dir: str, collection_name: str):
+        self.client = chromadb.Client(
+            settings=chromadb.Settings(
+                persist_directory=persist_dir,
+                anonymized_telemetry=False
+            )
+        )
+        self.collection = self.client.get_or_create_collection(
+            name=collection_name
+        )
+
+    def add(
+        self,
+        chunks: List[CodeChunk],
+        embeddings: List[List[float]]
+    ) -> None:
+        self.collection.add(
+            ids=[c.id for c in chunks],
+            documents=[c.content for c in chunks],
+            metadatas=[
+                {
+                    "file_path": c.file_path,
+                    "language": c.language,
+                    "start_line": c.start_line,
+                    "end_line": c.end_line,
+                    "symbol_type": c.symbol_type
+                }
+                for c in chunks
+            ],
+            embeddings=embeddings
+        )
+        self.client.persist()
```

---

### âœ… Verification Checklist

* [ ] One collection per project
* [ ] Metadata stored correctly
* [ ] Persist called
* [ ] No querying yet

---

## ğŸ§± TASK 1.12 â€” Index Orchestration Service

### ğŸ¯ Objective

Create the **single entry point** that runs the full indexing pipeline.

This is the heart of Phase 1.

---

### ğŸ§© Structured Patch Proposal

```json
{
  "taskId": "1.12",
  "title": "Implement indexing orchestration service",
  "files": [
    { "path": "server/indexing/service.py", "action": "create" }
  ],
  "requiresApproval": true,
  "riskLevel": "high"
}
```

---

### ğŸ§¾ Unified Diff

```diff
+++ server/indexing/service.py
+from pathlib import Path
+
+from .scanner import WorkspaceScanner
+from .language import detect_language
+from .chunker import SemanticChunker
+from .hash_tracker import hash_file
+from .state import IndexState
+from .vector_store import VectorStore
+
+
+class IndexingService:
+    def __init__(
+        self,
+        workspace: Path,
+        index_root: Path,
+        embedder,
+        progress=None
+    ):
+        self.workspace = workspace
+        self.index_root = index_root
+        self.embedder = embedder
+        self.progress = progress
+
+        self.scanner = WorkspaceScanner()
+        self.chunker = SemanticChunker()
+
+    def run(self) -> None:
+        state = IndexState(self.index_root)
+        state.load()
+
+        files = self.scanner.scan(self.workspace)
+
+        all_chunks = []
+        texts = []
+
+        for idx, path in enumerate(files, start=1):
+            if self.progress:
+                self.progress.report("scan", idx, len(files))
+
+            current_hash = hash_file(path)
+            stored_hash = state.file_hashes.get(str(path))
+
+            if stored_hash == current_hash:
+                continue
+
+            language = detect_language(path)
+            if not language:
+                continue
+
+            source = path.read_text(encoding="utf-8", errors="ignore")
+
+            chunks = self.chunker.chunk_file(
+                file_path=str(path),
+                language=language,
+                source=source
+            )
+
+            all_chunks.extend(chunks)
+            texts.extend([c.content for c in chunks])
+
+            state.file_hashes[str(path)] = current_hash
+
+        if not all_chunks:
+            return
+
+        embeddings = self.embedder.embed(texts)
+
+        store = VectorStore(
+            persist_dir=str(self.index_root / "chroma"),
+            collection_name="code_chunks"
+        )
+        store.add(all_chunks, embeddings)
+
+        state.save()
```

---

### âœ… Verification Checklist

* [ ] End-to-end pipeline runs
* [ ] Hash-based skipping works
* [ ] Index persists across restarts
* [ ] No querying
* [ ] No UI logic

---

## ğŸ‰ PHASE 1 â€” COMPLETE

You now have:

âœ… Deterministic indexing pipeline
âœ… Semantic chunking (safe fallback)
âœ… Hash-based smart sync
âœ… Persistent index state
âœ… Ollama embeddings
âœ… ChromaDB vector storage

All aligned with:

* privacy-first design
* MVP security constraints
* future v1.1 / v1.2 expansion

---

## updating the patch

Perfect â€” below is a **clean, ready-to-apply Phase 1 completion patch** that fixes **all blocking issues** and **formalizes Phase 1 as DONE**.

This patch is **minimal**, **safe**, and **does not introduce Phase 2 concepts**.

---

# âœ… Phase 1 Completion Patch

**Patch name:** `phase1-indexing-completion`
**Scope:** Server / indexing
**Fixes:**

* Progress reporting (blocking)
* Vector query support (blocking)
* Index completion signaling
* Explicit Phase-1 limitations documentation

---

## ğŸ¯ Phase 1 Acceptance Criteria (after patch)

* âœ” Indexing reports meaningful progress
* âœ” ChromaDB supports queries for Chat RAG
* âœ” Index lifecycle is explicit (complete phase)
* âœ” Chat backend can retrieve chunks
* âœ” No Phase-2 features added

---

# ğŸ§© PATCH CONTENT

---

## 1ï¸âƒ£ Fix Progress Reporting (BLOCKING)

### ğŸ“„ `server/indexing/progress.py` (UPDATE)

```python
from typing import Callable, Literal

Phase = Literal[
    "scan",
    "chunk",
    "embed",
    "store",
    "complete"
]


class ProgressTracker:
    def __init__(self, callback: Callable[[Phase, int, int], None]):
        self.callback = callback

    def report(self, phase: Phase, current: int, total: int) -> None:
        self.callback(phase, current, total)
```

âœ” Removes unused phases
âœ” Matches actual pipeline
âœ” UI-friendly

---

## 2ï¸âƒ£ Complete Indexing Lifecycle (BLOCKING)

### ğŸ“„ `server/indexing/service.py` (UPDATE)

```python
def run(self) -> None:
    state = IndexState(self.index_root)
    state.load()

    files = self.scanner.scan(self.workspace)
    total_files = len(files)

    all_chunks = []
    texts = []

    for idx, path in enumerate(files, start=1):
        if self.progress:
            self.progress.report("scan", idx, total_files)

        current_hash = hash_file(path)
        stored_hash = state.file_hashes.get(str(path))

        if stored_hash == current_hash:
            continue

        language = detect_language(path)
        if not language:
            continue

        source = path.read_text(encoding="utf-8", errors="ignore")

        chunks = self.chunker.chunk_file(
            file_path=str(path),
            language=language,
            source=source
        )

        all_chunks.extend(chunks)
        texts.extend([c.content for c in chunks])

        state.file_hashes[str(path)] = current_hash

        if self.progress:
            self.progress.report("chunk", idx, total_files)

    if not all_chunks:
        if self.progress:
            self.progress.report("complete", total_files, total_files)
        return

    if self.progress:
        self.progress.report("embed", 0, len(texts))

    embeddings = self.embedder.embed(texts)

    if self.progress:
        self.progress.report("store", 0, len(all_chunks))

    store = VectorStore(
        persist_dir=str(self.index_root / "chroma"),
        collection_name="code_chunks"
    )
    store.add(all_chunks, embeddings)

    state.save()

    if self.progress:
        self.progress.report("complete", total_files, total_files)
```

âœ” Explicit **complete phase**
âœ” Progress phases now accurate
âœ” Phase 1 UI can rely on them

---

## 3ï¸âƒ£ Add Vector Query Support (BLOCKING)

### ğŸ“„ `server/indexing/vector_store.py` (UPDATE)

```python
from typing import List, Optional
import chromadb

from .chunk import CodeChunk


class VectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        self.client = chromadb.Client(
            settings=chromadb.Settings(
                persist_directory=persist_dir,
                anonymized_telemetry=False
            )
        )
        self.collection = self.client.get_or_create_collection(
            name=collection_name
        )

    def add(
        self,
        chunks: List[CodeChunk],
        embeddings: List[List[float]]
    ) -> None:
        self.collection.add(
            ids=[c.id for c in chunks],
            documents=[c.content for c in chunks],
            metadatas=[
                {
                    "file_path": c.file_path,
                    "language": c.language,
                    "start_line": c.start_line,
                    "end_line": c.end_line,
                    "symbol_type": c.symbol_type
                }
                for c in chunks
            ],
            embeddings=embeddings
        )
        self.client.persist()

    def query(
        self,
        query_embedding: List[float],
        top_k: int = 5,
        where: Optional[dict] = None
    ) -> List[dict]:
        result = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where
        )

        hits = []
        for i in range(len(result["ids"][0])):
            hits.append({
                "id": result["ids"][0][i],
                "content": result["documents"][0][i],
                "metadata": result["metadatas"][0][i],
                "distance": result["distances"][0][i]
            })

        return hits
```

âœ” Enables Chat RAG
âœ” No Act/Plan logic added
âœ” Phase-1 compliant retrieval only

---

## 4ï¸âƒ£ Phase-1 Limitation Documentation (REQUIRED)

### ğŸ“„ `server/indexing/README.md` (NEW)

```md
# Indexing (Phase 1)

## Status
Phase 1 â€“ MVP indexing backend

## Supported
- Workspace scanning
- Hash-based incremental indexing
- Deterministic chunk IDs
- Ollama embeddings
- ChromaDB persistence
- Vector querying for Chat RAG

## Known Limitations (Expected)
- File-level chunking only (AST chunking in Phase 2)
- Deleted files are not yet purged from index
- One-request-per-embedding (batching later)

## Guarantees
- Index survives restart
- Same file â†’ same chunk ID
- Read-only (no workspace mutation)
```

âœ” Explicitly documents MVP boundaries
âœ” Prevents scope creep
âœ” Reviewer-safe

---

# âœ… FINAL PHASE 1 STATUS

After applying this patch:

ğŸŸ¢ **Phase 1 = COMPLETE**

You now have:

* A real, persistent RAG index
* Queryable embeddings
* Correct progress signaling
* Clean separation from Phase 2

---

## ğŸ”œ Next Optional Steps (do NOT apply yet)

* Phase 1.1: AST chunking
* Phase 2: Plan Mode
* Phase 2.1: Act Mode (writes)

If you want, I can next:

* Generate a **git commit message** (conventional)
* Create a **Phase 1 merge gate checklist**
* Prepare **Phase 2 patch roadmap**

Just tell me ğŸ‘

------------

Perfect. Below is **everything you need for Phase 1.1**, delivered in a clean, apply-ready form.

You will get **two things**:

1. âœ… **Phase 1.1 Patch** (Query Service + API endpoint)
2. ğŸ¤– **Phase 1.1 AI-Agent Prompt** (strict, reusable, scope-safe)

No Plan. No Act. No file writes. No shortcuts.

---

# âœ… PHASE 1.1 PATCH

**Name:** `phase1.1-query-api`
**Scope:** `server/` only
**Purpose:** Make Chat RAG work by retrieving indexed code

---

## 1ï¸âƒ£ New Query Service (Core of Phase 1.1)

### ğŸ“„ `server/indexing/query_service.py` **(NEW)**

```python
from pathlib import Path
from typing import List, Optional

from .vector_store import VectorStore


class QueryService:
    """
    Phase 1.1
    ----------
    Responsible for:
    - Embedding user query
    - Retrieving relevant code chunks from vector store
    - Returning raw retrieval results (no prompt logic)
    """

    def __init__(
        self,
        index_root: Path,
        embedder,
        collection_name: str = "code_chunks"
    ):
        self.embedder = embedder
        self.store = VectorStore(
            persist_dir=str(index_root / "chroma"),
            collection_name=collection_name
        )

    def query(
        self,
        text: str,
        top_k: int = 5,
        where: Optional[dict] = None
    ) -> List[dict]:
        embedding = self.embedder.embed([text])[0]

        return self.store.query(
            query_embedding=embedding,
            top_k=top_k,
            where=where
        )
```

âœ… Single responsibility
âœ… Deterministic behavior
âœ… Phase-1 safe

---

## 2ï¸âƒ£ Query API Schema

### ğŸ“„ `server/api/schemas/query.py` **(NEW)**

```python
from typing import List, Optional
from pydantic import BaseModel


class QueryRequest(BaseModel):
    project_id: str
    query: str
    top_k: int = 5
    filters: Optional[dict] = None


class RetrievedChunk(BaseModel):
    id: str
    content: str
    metadata: dict
    distance: float


class QueryResponse(BaseModel):
    chunks: List[RetrievedChunk]
```

---

## 3ï¸âƒ£ Query API Route

### ğŸ“„ `server/api/routes/query.py` **(NEW)**

```python
from pathlib import Path
from fastapi import APIRouter, Depends

from ..schemas.query import QueryRequest, QueryResponse
from ...indexing.query_service import QueryService
from ...dependencies import get_embedder, get_index_root

router = APIRouter()


@router.post("/query", response_model=QueryResponse)
def query_index(
    request: QueryRequest,
    embedder = Depends(get_embedder),
    index_root: Path = Depends(get_index_root),
):
    service = QueryService(
        index_root=index_root / request.project_id,
        embedder=embedder
    )

    results = service.query(
        text=request.query,
        top_k=request.top_k,
        where=request.filters
    )

    return {"chunks": results}
```

---

## 4ï¸âƒ£ Register Query Route

### ğŸ“„ `server/api/main.py` **(UPDATE)**

```python
from fastapi import FastAPI

from .routes import health, index, query

app = FastAPI()

app.include_router(health.router, prefix="/api")
app.include_router(index.router, prefix="/api")
app.include_router(query.router, prefix="/api")
```

---

## 5ï¸âƒ£ Dependency Wiring (if not already present)

### ğŸ“„ `server/api/dependencies.py` **(ADD if missing)**

```python
from pathlib import Path
from functools import lru_cache

from ..indexing.embeddings.ollama import OllamaEmbeddingProvider


@lru_cache()
def get_embedder():
    return OllamaEmbeddingProvider(
        base_url="http://localhost:11434",
        model="mxbai-embed-large"
    )


def get_index_root() -> Path:
    return Path.home() / ".localpilot" / "indexes"
```

---

## âœ… Phase 1.1 Server Patch Result

After this patch:

* âœ” Indexed code can be queried
* âœ” Chat backend has RAG context
* âœ” Retrieval is isolated & reusable
* âœ” No Plan / Act leakage
* âœ” Server is Phase-1.1 complete

---

# ğŸ¤– PHASE 1.1 AI-AGENT PROMPT

(copy-paste ready)

```
You are working on the LocalPilot project.

CURRENT PHASE:
Phase 1.1 â€“ Chat RAG Integration (READ-ONLY)

GOAL:
Connect the existing indexing system to a query API that retrieves relevant code chunks for chat.

STRICT RULES:
- DO NOT generate Plan Mode
- DO NOT generate Act Mode
- DO NOT write or modify workspace files
- DO NOT generate prompts for code creation
- DO NOT introduce execution or approval flows
- READ-ONLY indexing + retrieval ONLY

SCOPE:
Server-side only (Python)

YOU MUST:
1. Create a QueryService that:
   - Embeds user queries using the existing embedder
   - Queries ChromaDB via VectorStore
   - Returns raw retrieval results

2. Add a POST /api/query endpoint that:
   - Accepts query text and top_k
   - Calls QueryService
   - Returns retrieved chunks with metadata

3. Reuse existing infrastructure:
   - OllamaEmbeddingProvider
   - VectorStore
   - IndexState layout (~/.localpilot/indexes/{project_id})

4. Follow project architecture:
   - No business logic in routes
   - Services own behavior
   - Schemas define contracts

5. Keep everything deterministic and restart-safe.

SUCCESS CRITERIA:
- A user query returns relevant code chunks
- Results survive server restart
- No file system mutations occur
- Phase 2 features are NOT introduced

If unsure, STOP and ask before continuing.
```

---

# ğŸ§ª Phase 1.1 Completion Checklist

You can mark Phase 1.1 DONE when:

* [ ] Project indexed
* [ ] POST `/api/query` returns chunks
* [ ] Returned chunks reference real files
* [ ] Results stable across restarts
* [ ] No file writes possible

---

## ğŸŸ¢ What Comes Next (NOT NOW)

**Phase 1.2:** Chat streaming + prompt injection
**Phase 2:** Plan Mode
**Phase 3:** Act Mode

---

If you want, next I can:

* ğŸ”¹ Review your **actual implementation after applying this**
* ğŸ”¹ Generate **Phase 1.2 (Chat streaming) patch**
* ğŸ”¹ Prepare **Phase 2 kickoff checklist**

Just tell me what to do next.

----

Perfect â€” below is the **complete Phase 1.2 delivery**, exactly scoped and ready to apply.

Youâ€™ll get **two things**:

1. âœ… **Phase 1.2 Patch** â€” Chat orchestration + streaming (server-side)
2. ğŸ¤– **Phase 1.2 AI-Agent Prompt** â€” strict, reusable, phase-safe

No Plan.
No Act.
No file writes.
No scope creep.

---

# âœ… PHASE 1.2 PATCH

**Name:** `phase1.2-chat-streaming`
**Scope:** `server/`
**Purpose:** Turn retrieval into **streamed, RAG-aware chat**

---

## ğŸ§© Architecture Reminder (Phase 1.2)

```
User Question
   â†“
POST /api/query        (already done)
   â†“
Retrieved chunks
   â†“
WS /ws/chat
   â†“
Prompt = system + RAG context + user message
   â†“
Ollama /api/chat (streaming)
   â†“
Tokens streamed to client
```

---

## 1ï¸âƒ£ Chat Prompt Builder (RAG Injection)

### ğŸ“„ `server/chat/prompt_builder.py` **(NEW)**

````py
from typing import List, Dict


class PromptBuilder:
    """
    Phase 1.2
    ----------
    Builds a chat prompt with injected RAG context.
    No planning, no execution, no file writes.
    """

    SYSTEM_PROMPT = (
        "You are a helpful AI assistant answering questions about a codebase.\n"
        "You must base your answers ONLY on the provided code context.\n"
        "If the answer is not in the context, say you don't know.\n"
        "Do NOT suggest code changes or plans."
    )

    def build(
        self,
        user_message: str,
        chunks: List[Dict]
    ) -> List[Dict]:
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT}
        ]

        if chunks:
            context_blocks = []
            for c in chunks:
                meta = c.get("metadata", {})
                context_blocks.append(
                    f"File: {meta.get('file_path')} "
                    f"(lines {meta.get('start_line')}â€“{meta.get('end_line')})\n"
                    f"```{meta.get('language', '')}\n"
                    f"{c.get('content')}\n```"
                )

            messages.append({
                "role": "system",
                "content": "CODE CONTEXT:\n\n" + "\n\n".join(context_blocks)
            })

        messages.append({"role": "user", "content": user_message})
        return messages
````

âœ” Deterministic
âœ” Read-only
âœ” RAG-only
âœ” Phase-safe

---

## 2ï¸âƒ£ Ollama Chat Client (Streaming)

### ğŸ“„ `server/chat/ollama_chat_client.py` **(NEW)**

```py
import json
import requests
from typing import Iterable, Dict


class OllamaChatClient:
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url.rstrip("/")
        self.model = model

    def stream_chat(self, messages: Iterable[Dict]) -> Iterable[str]:
        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": list(messages),
                "stream": True
            },
            stream=True,
            timeout=300
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if not line:
                continue

            data = json.loads(line.decode("utf-8"))
            if "message" in data and "content" in data["message"]:
                yield data["message"]["content"]
```

âœ” Uses Ollama streaming
âœ” No buffering
âœ” Token-by-token yield

---

## 3ï¸âƒ£ Chat Service (Orchestration)

### ğŸ“„ `server/chat/chat_service.py` **(NEW)**

```py
from pathlib import Path
from typing import Iterable

from .prompt_builder import PromptBuilder
from .ollama_chat_client import OllamaChatClient
from ..indexing.query_service import QueryService


class ChatService:
    """
    Phase 1.2
    ----------
    Orchestrates:
    - RAG retrieval
    - Prompt building
    - Streaming chat response
    """

    def __init__(
        self,
        index_root: Path,
        embedder,
        ollama_base_url: str,
        chat_model: str
    ):
        self.query_service = QueryService(
            index_root=index_root,
            embedder=embedder
        )
        self.prompt_builder = PromptBuilder()
        self.chat_client = OllamaChatClient(
            base_url=ollama_base_url,
            model=chat_model
        )

    def stream_chat(
        self,
        project_id: str,
        user_message: str,
        top_k: int = 5
    ) -> Iterable[str]:
        chunks = self.query_service.query(
            text=user_message,
            top_k=top_k
        )

        messages = self.prompt_builder.build(
            user_message=user_message,
            chunks=chunks
        )

        return self.chat_client.stream_chat(messages)
```

âœ” Clean separation
âœ” No UI logic
âœ” No Plan/Act logic

---

## 4ï¸âƒ£ WebSocket Chat Endpoint

### ğŸ“„ `server/api/routes/chat_ws.py` **(NEW)**

```py
from pathlib import Path
from fastapi import APIRouter, WebSocket, Depends

from ...chat.chat_service import ChatService
from ..dependencies import get_embedder, get_index_root

router = APIRouter()


@router.websocket("/ws/chat")
async def chat_ws(
    websocket: WebSocket,
    embedder = Depends(get_embedder),
    index_root: Path = Depends(get_index_root),
):
    await websocket.accept()
    payload = await websocket.receive_json()

    service = ChatService(
        index_root=index_root / payload["project_id"],
        embedder=embedder,
        ollama_base_url="http://localhost:11434",
        chat_model=payload.get("model", "qwen2.5-coder")
    )

    try:
        for token in service.stream_chat(
            project_id=payload["project_id"],
            user_message=payload["message"],
            top_k=payload.get("top_k", 5)
        ):
            await websocket.send_text(token)
    finally:
        await websocket.close()
```

âœ” Streaming WebSocket
âœ” Minimal protocol
âœ” Phase-1.2 compliant

---

## 5ï¸âƒ£ Register Chat WS Route

### ğŸ“„ `server/main.py` **(UPDATE)**

```py
from fastapi import FastAPI

from .api.routes import query as query_routes
from .api.routes import chat_ws

app = FastAPI()

app.include_router(query_routes.router, prefix="/api")
app.include_router(chat_ws.router)

@app.get("/health")
def health():
    return {"status": "ok"}
```

---

# âœ… Phase 1.2 Patch Result

After applying this patch:

* âœ” Chat responses stream token-by-token
* âœ” Answers are grounded in indexed code
* âœ” No file writes possible
* âœ” Restart safe
* âœ” Phase 1 is now **fully complete**

---

# ğŸ¤– PHASE 1.2 AI-AGENT PROMPT

(copy-paste ready)

```
You are working on the LocalPilot project.

CURRENT PHASE:
Phase 1.2 â€“ Chat Streaming with RAG (READ-ONLY)

GOAL:
Enable a streamed chat experience that answers questions using retrieved code context.

STRICT RULES:
- DO NOT generate Plan Mode
- DO NOT generate Act Mode
- DO NOT modify workspace files
- DO NOT suggest code changes
- DO NOT execute commands
- READ-ONLY chat ONLY

SCOPE:
Server-side Python only.

YOU MUST:
1. Retrieve relevant code chunks using the existing QueryService.
2. Inject retrieved chunks into a chat prompt with file paths and line numbers.
3. Call Ollamaâ€™s /api/chat endpoint with streaming enabled.
4. Stream tokens to the client via WebSocket (/ws/chat).
5. Keep prompt logic, retrieval logic, and transport logic separated.

ARCHITECTURE RULES:
- Routes do NOT contain business logic.
- Services orchestrate behavior.
- Prompt building is isolated.
- Deterministic, restart-safe behavior only.

SUCCESS CRITERIA:
- User asks a question.
- Relevant code is retrieved.
- LLM answers reference real code.
- Tokens stream live.
- No file writes or plans are possible.

If unsure, STOP and ask before continuing.
```

---

# ğŸŸ¢ PROJECT STATUS AFTER THIS

### âœ… **PHASE 1 â€” COMPLETE**

You now have:

* Indexing
* Incremental sync
* Querying
* RAG-aware chat
* Streaming responses

---

## ğŸ”œ What Comes Next (Only When Youâ€™re Ready)

ğŸ‘‰ **Phase 2 â€” Plan Mode**
(Structured TODO generation, still no execution)

If you want, next I can:

* Review your Phase 1.2 implementation after apply
* Generate Phase 2.0 Plan Mode patch
* Prepare a Phase 1 â†’ Phase 2 transition gate

Just tell me the next move.
